{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9cfBPDJfUfd",
        "outputId": "41155158-4429-453b-accd-12386f2297ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17U8xUevAGgm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch import linalg as LA\n",
        "import time\n",
        "dir = '/content/drive/MyDrive/SIMSIAM/Experiments'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdJQxhd5NpFT"
      },
      "source": [
        "# Defining SiamSim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NQTh64pB1Qb"
      },
      "outputs": [],
      "source": [
        "class SiamSimModel(nn.Module):\n",
        "    def __init__(self, encoder, dim, pred_dim, last_dim, spec_norm=False):\n",
        "        super().__init__()\n",
        "        #conditions:\n",
        "        self.ldim = last_dim\n",
        "\n",
        "        #encoder\n",
        "        self.encoder = encoder\n",
        "\n",
        "        \n",
        "        #projectionMLP\n",
        "        self.projector = nn.Sequential(nn.Linear(last_dim, last_dim, bias=False),\n",
        "                                      nn.BatchNorm1d(last_dim),\n",
        "                                      nn.ReLU(inplace=True),\n",
        "                                      nn.Linear(last_dim, last_dim, bias=False),\n",
        "                                      nn.BatchNorm1d(last_dim),\n",
        "                                      nn.ReLU(inplace=True),\n",
        "                                      nn.Linear(last_dim, dim, bias=False),\n",
        "                                      nn.BatchNorm1d(dim))        \n",
        "        \n",
        "          \n",
        "        \n",
        "        if spec_norm:\n",
        "          #predictionMLP\n",
        "          self.predictor = nn.Sequential(nn.utils.parametrizations.spectral_norm(nn.Linear(dim, pred_dim, bias=False)),\n",
        "                                        nn.BatchNorm1d(pred_dim),\n",
        "                                        nn.ReLU(inplace=True),\n",
        "                                        nn.utils.parametrizations.spectral_norm(nn.Linear(pred_dim, dim)))\n",
        "          \n",
        "        else:\n",
        "          #predictionMLP\n",
        "          self.predictor = nn.Sequential(nn.Linear(dim, pred_dim, bias=False),\n",
        "                                        nn.BatchNorm1d(pred_dim),\n",
        "                                        nn.ReLU(inplace=True),\n",
        "                                        nn.Linear(pred_dim, dim))\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        z1 = self.projector(self.encoder(x1).view(-1, self.ldim))\n",
        "        z2 = self.projector(self.encoder(x2).view(-1, self.ldim))\n",
        "        p1 = self.predictor(z1)\n",
        "        p2 = self.predictor(z2)    \n",
        "        return p1, p2, z1.detach(), z2.detach()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(dis)similarity functions\n",
        "CosSim = nn.CosineSimilarity(dim=1)\n",
        "SoftMax = nn.Softmax(dim=1)\n",
        "LogSoftMax = nn.LogSoftmax(dim=1)\n",
        "MSE = nn.MSELoss()\n",
        "pd_2 = nn.PairwiseDistance(p=2)\n",
        "pd_1 = nn.PairwiseDistance(p=1)\n",
        "pd_inf = nn.PairwiseDistance(p=torch.inf)\n",
        "\n",
        "def neg_cosine_sim(a,b):\n",
        "  return -CosSim(a, b)\n",
        "\n",
        "def cross_entropy_sim(a,b):\n",
        "  return -SoftMax(b)*LogSoftMax(a)\n",
        "\n",
        "def mse_sim(a,b):\n",
        "  return nn.MSELoss()\n",
        "\n",
        "def pdist_1(a,b):\n",
        "  return pd_1(a, b)\n",
        "\n",
        "def pdist_2(a,b):\n",
        "  return pd_2(a, b)\n",
        "\n",
        "def pdist_inf(a,b):\n",
        "  return pd_inf(a,b)"
      ],
      "metadata": {
        "id": "kGwEPx-WWC0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmlKyNNRLMyn"
      },
      "outputs": [],
      "source": [
        "dim=2048\n",
        "pred_dim = 512\n",
        "backbone = models.resnet18()\n",
        "last_dim=list(backbone.children())[-1].in_features\n",
        "\n",
        "encoder = torch.nn.Sequential(*(list(backbone.children())[:-1]))\n",
        "\n",
        "#EXPERIMENTS\n",
        "opt='default'#'avg'#'adam' #1. OPTIMIZERS:\n",
        "sim_fun_flag='default'#'p2'#'pinf' #2. SIMILARITY FUNCTIONS\n",
        "loss_agg='default'#'geo' #3. LOSS AGGREGATIONS:\n",
        "spec_norm = False #4. REGULARIZERS (SPEC NORM)\n",
        "\n",
        "model = SiamSimModel(encoder, dim, pred_dim, last_dim, spec_norm=spec_norm)\n",
        "\n",
        "#EXPERIMENT 2 - MORE SIM FUNS\n",
        "if sim_fun_flag=='default':\n",
        "  sim_fun = cross_entropy_sim\n",
        "elif sim_fun_flag=='p2':\n",
        "  sim_fun = pdist_2\n",
        "elif sim_fun_flag=='pinf':\n",
        "  sim_fun = pdist_inf\n",
        "\n",
        "if spec_norm:\n",
        "  param_groups = [{'params': model.encoder.parameters(), 'name': 'encoder'},\n",
        "                  {'params': model.projector.parameters(), 'name': 'projector'},\n",
        "                  {'params': model.predictor.parameters(), 'weight_decay': 0, 'name': 'predictor'}]\n",
        "else:\n",
        "  param_groups = [{'params': model.encoder.parameters(), 'name': 'encoder'},\n",
        "                  {'params': model.projector.parameters(), 'name': 'projector'},\n",
        "                  {'params': model.predictor.parameters(), 'name': 'predictor'}]\n",
        "\n",
        "#training settings:\n",
        "batch_size = 256\n",
        "num_epochs = 20#100\n",
        "lr = 0.05*(batch_size/256)\n",
        "if opt=='default':\n",
        "  optimizer = torch.optim.SGD(param_groups, lr=lr, momentum=0.9, weight_decay=1e-4)\n",
        "elif opt=='avg'\n",
        "  optimizer = torch.optim.ASGD(param_groups, lr=lr, momentum=0.9, weight_decay=1e-4)\n",
        "elif opt=='adam':\n",
        "  optimizer = torch.optim.Adam(param_groups, lr=lr, betas=(0.9, 0.999), weight_decay=1e-4)\n",
        "\n",
        "device = torch.device('cuda')# if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#name of the experiment to perform\n",
        "setup = opt + '_' + sim_fun_flag + '_' + loss_agg\n",
        "if spec_norm:\n",
        "  setup = setup + '_specnorm'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cosine schedule for LR\n",
        "import math\n",
        "def update_lr(optimizer, current_e, total_e, max_lr, min_lr=0, spec_norm=False):\n",
        "  for g in optimizer.param_groups:\n",
        "    if g['name'] == 'predictor':\n",
        "      pass\n",
        "    else:\n",
        "      g['lr'] = min_lr + 0.5*(max_lr-min_lr)*(1  + math.cos((math.pi*current_e)/total_e))"
      ],
      "metadata": {
        "id": "7SulEKn34AoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WX5GL4_xNqkd"
      },
      "source": [
        "# Defining Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y5WXGMdCBW2",
        "outputId": "042b2761-b47b-431e-aa81-3ad59412b121"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from PIL import ImageFilter\n",
        "import random\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "\n",
        "traindir = \"data\"\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "\n",
        "class GaussianBlur(object):\n",
        "    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n",
        "\n",
        "    def __init__(self, sigma=[.1, 2.]):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def __call__(self, x):\n",
        "        sigma = random.uniform(self.sigma[0], self.sigma[1])\n",
        "        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))\n",
        "        return x\n",
        "\n",
        "\n",
        "augmentation = [\n",
        "        transforms.RandomResizedCrop(224, scale=(0.2, 1.)),\n",
        "        transforms.RandomApply([\n",
        "            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  # not strengthened\n",
        "        ], p=0.8),\n",
        "        transforms.RandomGrayscale(p=0.2),\n",
        "        transforms.RandomApply([GaussianBlur([.1, 2.])], p=0.5),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]\n",
        "\n",
        "class TwoCropsTransform:\n",
        "    \"\"\"Take two random crops of one image as the query and key.\"\"\"\n",
        "\n",
        "    def __init__(self, base_transform):\n",
        "        self.base_transform = base_transform\n",
        "\n",
        "    def __call__(self, x):\n",
        "        q = self.base_transform(x)\n",
        "        k = self.base_transform(x)\n",
        "        return [q, k]\n",
        "\n",
        "tranform = TwoCropsTransform(transforms.Compose(augmentation))\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                        download=True, transform=tranform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size, shuffle=True,\n",
        "        num_workers=2, pin_memory=True, sampler=None, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTxsvr6xNyCK"
      },
      "source": [
        "# Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "X2EQJ1OsK-vT",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "6916ea7a-efb7-495a-b1fa-0bb84a37b842",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=0/100 step=0/781 loss=0.003763296641409397 time=1.0193977355957031 secs\n",
            "epoch=0/100 step=10/781 loss=0.003764156252145767 time=1.14306640625 secs\n",
            "epoch=0/100 step=20/781 loss=0.0037637464702129364 time=1.233649730682373 secs\n",
            "epoch=0/100 step=30/781 loss=0.00376409525051713 time=1.0110406875610352 secs\n",
            "epoch=0/100 step=40/781 loss=0.0037634908221662045 time=1.0809943675994873 secs\n",
            "epoch=0/100 step=50/781 loss=0.003762795589864254 time=1.0188500881195068 secs\n",
            "epoch=0/100 step=60/781 loss=0.0037636999040842056 time=1.0053744316101074 secs\n",
            "epoch=0/100 step=70/781 loss=0.003763706423342228 time=1.0113224983215332 secs\n",
            "epoch=0/100 step=80/781 loss=0.0037635217886418104 time=1.0095503330230713 secs\n",
            "epoch=0/100 step=90/781 loss=0.003763469634577632 time=1.0095727443695068 secs\n",
            "epoch=0/100 step=100/781 loss=0.003763496642932296 time=1.0090186595916748 secs\n",
            "epoch=0/100 step=110/781 loss=0.0037637169007211924 time=1.02461576461792 secs\n",
            "epoch=0/100 step=120/781 loss=0.0037635681219398975 time=1.0562658309936523 secs\n",
            "epoch=0/100 step=130/781 loss=0.00376289919950068 time=1.1238629817962646 secs\n",
            "epoch=0/100 step=140/781 loss=0.003762683365494013 time=1.2242162227630615 secs\n",
            "epoch=0/100 step=150/781 loss=0.0037636891938745975 time=1.0070140361785889 secs\n",
            "epoch=0/100 step=160/781 loss=0.0037643960677087307 time=1.1358332633972168 secs\n",
            "epoch=0/100 step=170/781 loss=0.003764263354241848 time=1.0883564949035645 secs\n",
            "epoch=0/100 step=180/781 loss=0.0037628961727023125 time=1.1436328887939453 secs\n",
            "epoch=0/100 step=190/781 loss=0.00376346567645669 time=1.361708164215088 secs\n",
            "epoch=0/100 step=200/781 loss=0.0037629837170243263 time=1.4941370487213135 secs\n",
            "epoch=0/100 step=210/781 loss=0.003763502463698387 time=1.666360855102539 secs\n",
            "epoch=0/100 step=220/781 loss=0.003762361127883196 time=1.6747193336486816 secs\n",
            "epoch=0/100 step=230/781 loss=0.003764023305848241 time=1.6727676391601562 secs\n",
            "epoch=0/100 step=240/781 loss=0.0037631350569427013 time=1.6692514419555664 secs\n",
            "epoch=0/100 step=250/781 loss=0.0037643704563379288 time=1.6688072681427002 secs\n",
            "epoch=0/100 step=260/781 loss=0.0037633543834090233 time=1.6708507537841797 secs\n",
            "epoch=0/100 step=270/781 loss=0.0037623243406414986 time=1.6711156368255615 secs\n",
            "epoch=0/100 step=280/781 loss=0.00376346567645669 time=1.6708471775054932 secs\n",
            "epoch=0/100 step=290/781 loss=0.0037622982636094093 time=1.6767423152923584 secs\n",
            "epoch=0/100 step=300/781 loss=0.0037626794073730707 time=1.6727330684661865 secs\n",
            "epoch=0/100 step=310/781 loss=0.0037629413418471813 time=1.6424124240875244 secs\n",
            "epoch=0/100 step=320/781 loss=0.0037621832452714443 time=1.6733150482177734 secs\n",
            "epoch=0/100 step=330/781 loss=0.003763241460546851 time=1.6696279048919678 secs\n",
            "epoch=0/100 step=340/781 loss=0.0037627648562192917 time=1.6658987998962402 secs\n",
            "epoch=0/100 step=350/781 loss=0.003761795349419117 time=1.671851634979248 secs\n",
            "epoch=0/100 step=360/781 loss=0.0037631732411682606 time=1.6814360618591309 secs\n",
            "epoch=0/100 step=370/781 loss=0.003762424923479557 time=1.6630468368530273 secs\n",
            "epoch=0/100 step=380/781 loss=0.003762630047276616 time=1.6714224815368652 secs\n",
            "epoch=0/100 step=390/781 loss=0.0037623881362378597 time=1.6794350147247314 secs\n",
            "epoch=0/100 step=400/781 loss=0.0037626009434461594 time=1.6750311851501465 secs\n",
            "epoch=0/100 step=410/781 loss=0.0037621150258928537 time=1.6685986518859863 secs\n",
            "epoch=0/100 step=420/781 loss=0.003762722946703434 time=1.667203426361084 secs\n",
            "epoch=0/100 step=430/781 loss=0.0037624298129230738 time=1.673553466796875 secs\n",
            "epoch=0/100 step=440/781 loss=0.003762664971873164 time=1.6741015911102295 secs\n",
            "epoch=0/100 step=450/781 loss=0.0037625390104949474 time=1.6705572605133057 secs\n",
            "epoch=0/100 step=460/781 loss=0.0037633974570780993 time=1.6706531047821045 secs\n",
            "epoch=0/100 step=470/781 loss=0.0037623406387865543 time=1.67246675491333 secs\n",
            "epoch=0/100 step=480/781 loss=0.003761384403333068 time=1.6738274097442627 secs\n",
            "epoch=0/100 step=490/781 loss=0.0037621348164975643 time=1.6743862628936768 secs\n",
            "epoch=0/100 step=500/781 loss=0.003761621890589595 time=1.6696867942810059 secs\n",
            "epoch=0/100 step=510/781 loss=0.003761355997994542 time=1.6680903434753418 secs\n",
            "epoch=0/100 step=520/781 loss=0.003762842621654272 time=1.6834564208984375 secs\n",
            "epoch=0/100 step=530/781 loss=0.0037614768370985985 time=1.676135540008545 secs\n",
            "epoch=0/100 step=540/781 loss=0.0037627010606229305 time=1.6710617542266846 secs\n",
            "epoch=0/100 step=550/781 loss=0.003762447740882635 time=1.6749558448791504 secs\n",
            "epoch=0/100 step=560/781 loss=0.003762427717447281 time=1.5527524948120117 secs\n",
            "epoch=0/100 step=570/781 loss=0.003762016538530588 time=1.4655752182006836 secs\n",
            "epoch=0/100 step=580/781 loss=0.0037616677582263947 time=1.285092830657959 secs\n",
            "epoch=0/100 step=590/781 loss=0.003761746222153306 time=1.5511035919189453 secs\n",
            "epoch=0/100 step=600/781 loss=0.003761386265978217 time=1.6746575832366943 secs\n",
            "epoch=0/100 step=610/781 loss=0.0037614363245666027 time=1.6729609966278076 secs\n",
            "epoch=0/100 step=620/781 loss=0.0037622840609401464 time=1.6711745262145996 secs\n",
            "epoch=0/100 step=630/781 loss=0.003761911764740944 time=1.6676247119903564 secs\n",
            "epoch=0/100 step=640/781 loss=0.00376182165928185 time=1.6753036975860596 secs\n",
            "epoch=0/100 step=650/781 loss=0.003761579282581806 time=1.6737377643585205 secs\n",
            "epoch=0/100 step=660/781 loss=0.0037620235234498978 time=1.6363062858581543 secs\n",
            "epoch=0/100 step=670/781 loss=0.003762617241591215 time=1.6661694049835205 secs\n",
            "epoch=0/100 step=680/781 loss=0.0037623418029397726 time=1.6761810779571533 secs\n",
            "epoch=0/100 step=690/781 loss=0.0037617553025484085 time=1.6797833442687988 secs\n",
            "epoch=0/100 step=700/781 loss=0.0037625590339303017 time=1.6657428741455078 secs\n",
            "epoch=0/100 step=710/781 loss=0.003761350642889738 time=1.6730856895446777 secs\n",
            "epoch=0/100 step=720/781 loss=0.003761297557502985 time=1.68131685256958 secs\n",
            "epoch=0/100 step=730/781 loss=0.0037620726507157087 time=1.6043367385864258 secs\n",
            "epoch=0/100 step=740/781 loss=0.0037605855613946915 time=1.5124013423919678 secs\n",
            "epoch=0/100 step=750/781 loss=0.003761801403015852 time=1.6695246696472168 secs\n",
            "epoch=0/100 step=760/781 loss=0.0037621548399329185 time=1.6706428527832031 secs\n",
            "epoch=0/100 step=770/781 loss=0.0037614982575178146 time=1.28483247756958 secs\n",
            "epoch=0/100 step=780/781 loss=0.0037619536742568016 time=1.6526150703430176 secs\n",
            "epoch=1/100 step=0/781 loss=0.0037623224779963493 time=0.9954836368560791 secs\n",
            "epoch=1/100 step=10/781 loss=0.0037615522742271423 time=1.2842655181884766 secs\n",
            "epoch=1/100 step=20/781 loss=0.003761528991162777 time=1.246288537979126 secs\n",
            "epoch=1/100 step=30/781 loss=0.0037612738087773323 time=1.2209827899932861 secs\n",
            "epoch=1/100 step=40/781 loss=0.0037620654329657555 time=1.2383081912994385 secs\n",
            "epoch=1/100 step=50/781 loss=0.003761033993214369 time=1.0333926677703857 secs\n",
            "epoch=1/100 step=60/781 loss=0.0037623224779963493 time=1.2851488590240479 secs\n",
            "epoch=1/100 step=70/781 loss=0.0037619471549987793 time=1.0532417297363281 secs\n",
            "epoch=1/100 step=80/781 loss=0.003761385567486286 time=0.9954259395599365 secs\n",
            "epoch=1/100 step=90/781 loss=0.003761149710044265 time=1.0763392448425293 secs\n",
            "epoch=1/100 step=100/781 loss=0.0037607639096677303 time=1.1908299922943115 secs\n",
            "epoch=1/100 step=110/781 loss=0.0037616174668073654 time=1.228775978088379 secs\n",
            "epoch=1/100 step=120/781 loss=0.0037612379528582096 time=1.5169668197631836 secs\n",
            "epoch=1/100 step=130/781 loss=0.0037607832346111536 time=1.2975389957427979 secs\n",
            "epoch=1/100 step=140/781 loss=0.0037610228173434734 time=1.5744361877441406 secs\n",
            "epoch=1/100 step=150/781 loss=0.0037610374856740236 time=1.657015085220337 secs\n",
            "epoch=1/100 step=160/781 loss=0.0037620235234498978 time=1.681549072265625 secs\n",
            "epoch=1/100 step=170/781 loss=0.003761181840673089 time=1.6599094867706299 secs\n",
            "epoch=1/100 step=180/781 loss=0.0037613653112202883 time=1.6542119979858398 secs\n",
            "epoch=1/100 step=190/781 loss=0.0037612849846482277 time=1.4375946521759033 secs\n",
            "epoch=1/100 step=200/781 loss=0.0037611937150359154 time=1.3064539432525635 secs\n",
            "epoch=1/100 step=210/781 loss=0.0037619573995471 time=1.5350265502929688 secs\n",
            "epoch=1/100 step=220/781 loss=0.0037617525085806847 time=1.6603410243988037 secs\n",
            "epoch=1/100 step=230/781 loss=0.0037604435347020626 time=1.499051570892334 secs\n",
            "epoch=1/100 step=240/781 loss=0.003761264495551586 time=1.6607885360717773 secs\n",
            "epoch=1/100 step=250/781 loss=0.0037607471458613873 time=1.4297542572021484 secs\n",
            "epoch=1/100 step=260/781 loss=0.003760694758966565 time=1.6559669971466064 secs\n",
            "epoch=1/100 step=270/781 loss=0.0037604006938636303 time=1.5083632469177246 secs\n",
            "epoch=1/100 step=280/781 loss=0.003760181600227952 time=1.6544387340545654 secs\n",
            "epoch=1/100 step=290/781 loss=0.003761152271181345 time=1.6534242630004883 secs\n",
            "epoch=1/100 step=300/781 loss=0.003760525956749916 time=1.6596827507019043 secs\n",
            "epoch=1/100 step=310/781 loss=0.003760932944715023 time=1.6551668643951416 secs\n",
            "epoch=1/100 step=320/781 loss=0.0037598018534481525 time=1.6634325981140137 secs\n",
            "epoch=1/100 step=330/781 loss=0.0037609850987792015 time=1.6632118225097656 secs\n",
            "epoch=1/100 step=340/781 loss=0.0037612419109791517 time=1.6728003025054932 secs\n",
            "epoch=1/100 step=350/781 loss=0.0037604537792503834 time=1.658797264099121 secs\n",
            "epoch=1/100 step=360/781 loss=0.0037608700804412365 time=1.6654129028320312 secs\n",
            "epoch=1/100 step=370/781 loss=0.003761127358302474 time=1.6547586917877197 secs\n",
            "epoch=1/100 step=380/781 loss=0.0037610691506415606 time=1.6514451503753662 secs\n",
            "epoch=1/100 step=390/781 loss=0.0037616961635649204 time=1.661170482635498 secs\n",
            "epoch=1/100 step=400/781 loss=0.0037609832361340523 time=1.662879228591919 secs\n",
            "epoch=1/100 step=410/781 loss=0.0037607785779982805 time=1.6891658306121826 secs\n",
            "epoch=1/100 step=420/781 loss=0.0037601618096232414 time=1.6572563648223877 secs\n",
            "epoch=1/100 step=430/781 loss=0.0037610549479722977 time=1.6534478664398193 secs\n",
            "epoch=1/100 step=440/781 loss=0.0037610530853271484 time=1.6619391441345215 secs\n",
            "epoch=1/100 step=450/781 loss=0.003759854007512331 time=1.66361665725708 secs\n",
            "epoch=1/100 step=460/781 loss=0.0037599618081003428 time=1.6563146114349365 secs\n",
            "epoch=1/100 step=470/781 loss=0.003759847953915596 time=1.6547772884368896 secs\n",
            "epoch=1/100 step=480/781 loss=0.003759910585358739 time=1.6562318801879883 secs\n",
            "epoch=1/100 step=490/781 loss=0.0037598174531012774 time=1.6574842929840088 secs\n",
            "epoch=1/100 step=500/781 loss=0.0037588877603411674 time=1.6433289051055908 secs\n",
            "epoch=1/100 step=510/781 loss=0.0037598926573991776 time=1.6636087894439697 secs\n",
            "epoch=1/100 step=520/781 loss=0.00375987496227026 time=1.6473281383514404 secs\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-e20204395dd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0masymmetric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = model.to(device)\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss, epoch_count = 0, 0\n",
        "    update_lr(optimizer=optimizer, current_e=epoch, \n",
        "              total_e=num_epochs, max_lr=lr)\n",
        "    for i, (images, _) in enumerate(train_loader):\n",
        "        start = time.time()\n",
        "        x1, x2 = images\n",
        "        x1, x2 = x1.to(device), x2.to(device)\n",
        "        p1, p2, z1, z2 = model(x1, x2)\n",
        "        if loss_agg=='geo':\n",
        "          pass#use geometric mean here!\n",
        "        else:\n",
        "          loss = (sim_fun(p1, z2).mean() + sim_fun(p2, z1).mean()) * 0.5\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss\n",
        "        epoch_count += 1\n",
        "        if(i%10==0):\n",
        "          message = f\"epoch={epoch}/{num_epochs} step={i}/{len(train_loader)} loss={loss} time={time.time()-start} secs\"\n",
        "          print(message)\n",
        "    epoch_loss /= epoch_count\n",
        "    print(\"train loss: {}\".format(epoch_loss))\n",
        "    if True:#epoch>=num_epochs-1 or epoch%20==0:\n",
        "      torch.save({\n",
        "          'epoch': epoch,\n",
        "          'model_state_dict': model.encoder.state_dict(),\n",
        "          'proj_state_dict': model.projector.state_dict(),\n",
        "          'pred_state_dict': model.predictor.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'loss': loss,\n",
        "          }, dir + f\"{setup}_checkpoint_{epoch}.pth\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SimSiamExperimentsCode.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}